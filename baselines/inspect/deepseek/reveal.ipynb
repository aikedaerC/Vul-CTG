{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f2e284d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clang import *\n",
    "from tokenizers import NormalizedString,PreTokenizedString\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers import Tokenizer, normalizers\n",
    "from tokenizers.pre_tokenizers import PreTokenizer\n",
    "from tokenizers.normalizers import StripAccents,Replace\n",
    "from tokenizers import processors\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "import threading\n",
    "from typing import List\n",
    "\n",
    "class MyTokenizer:\n",
    "    cidx = cindex.Index.create()\n",
    "\n",
    "    def __init__(self, timeout=5):  # 设置超时时间\n",
    "        self.timeout = timeout\n",
    "\n",
    "    def clang_split(self, i: int, normalized_string: NormalizedString) -> List[NormalizedString]:\n",
    "        \"\"\"\n",
    "        使用 Clang 对代码进行分词，增加超时机制\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        exception = None\n",
    "\n",
    "        def parse():\n",
    "            nonlocal result, exception\n",
    "            try:\n",
    "                tok = []\n",
    "                tu = self.cidx.parse(\n",
    "                    'tmp.c',\n",
    "                    args=[''],  \n",
    "                    unsaved_files=[('tmp.c', str(normalized_string.original))],  \n",
    "                    options=0\n",
    "                )\n",
    "                for t in tu.get_tokens(extent=tu.cursor.extent):\n",
    "                    spelling = t.spelling.strip()\n",
    "                    if spelling == '':\n",
    "                        continue\n",
    "                    tok.append(NormalizedString(spelling))\n",
    "                result = tok\n",
    "            except Exception as e:\n",
    "                exception = e\n",
    "\n",
    "        # 创建线程\n",
    "        thread = threading.Thread(target=parse)\n",
    "        thread.start()\n",
    "        thread.join(self.timeout)  # 等待超时时间\n",
    "\n",
    "        if thread.is_alive():  # 超时检查\n",
    "            print(f\"Timeout occurred while parsing: {normalized_string.original[:100]}...\")\n",
    "            thread.join(0)  # 跳过此任务\n",
    "            return []\n",
    "        if exception:\n",
    "            print(f\"Error during Clang parsing: {exception}\")\n",
    "            return []\n",
    "\n",
    "        return result\n",
    "\n",
    "    def pre_tokenize(self, pretok: PreTokenizedString):\n",
    "        \"\"\"\n",
    "        对预分词字符串进行处理，调用 Clang 分词器\n",
    "        \"\"\"\n",
    "        def preprocess_and_split(i: int, normalized_string: NormalizedString) -> List[NormalizedString]:\n",
    "            return self.clang_split(i, normalized_string)\n",
    "        \n",
    "        pretok.split(preprocess_and_split)\n",
    "\n",
    "import re\n",
    "def cleaner(code):\n",
    "    pat = re.compile(r'(/\\*([^*]|(\\*+[^*/]))*\\*+/)|(//.*)')\n",
    "    code = re.sub(pat, '', code)\n",
    "    code = re.sub('\\n', '', code)\n",
    "    code = re.sub('\\t', '', code)\n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb49c68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = \"reveal\"\n",
    "m1 = pd.read_pickle(f'../../data/finetune/{dataset}/{dataset}_train.pkl')\n",
    "m2 = pd.read_pickle(f'../../data/finetune/{dataset}/{dataset}_val.pkl')\n",
    "m3 = pd.read_pickle(f'../../data/finetune/{dataset}/{dataset}_test.pkl')\n",
    "\n",
    "for df in [m1, m2, m3]:\n",
    "    if \"functionSource\" in df.columns:\n",
    "        df[\"func\"] = df[\"functionSource\"].apply(cleaner)\n",
    "        \n",
    "    if dataset == \"draper\":\n",
    "        df[\"target\"] = df[\"combine\"] * 1\n",
    "\n",
    "    if \"label\" in df.columns:\n",
    "        df[\"target\"] = df[\"label\"]\n",
    "\n",
    "    if dataset == \"mvd\":\n",
    "        df[\"target\"] = df[\"target\"].apply(lambda x: 1 if x != 0 else 0)\n",
    "\n",
    "m1 = m1[[\"func\", \"target\"]]\n",
    "m2 = m2[[\"func\", \"target\"]]\n",
    "m3 = m3[[\"func\", \"target\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "812b38d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "func",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "target",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "fd14b7ce-ec45-4953-9a53-52d14e5f7a85",
       "rows": [
        [
         "11902",
         "OM_uint32 kg_sync_ccache_name ( krb5_context context , OM_uint32 * minor_status ) { OM_uint32 err = 0 ; if ( ! err ) { err = krb5_cc_set_default_name ( context , ( char * ) k5_getspecific ( K5_KEY_GSS_KRB5_CCACHE_NAME ) ) ; } * minor_status = err ; return ( * minor_status == 0 ) ? GSS_S_COMPLETE : GSS_S_FAILURE ; }",
         "0"
        ],
        [
         "11648",
         "static void write_bootloader ( CPUMIPSState * env , uint8_t * base , int64_t kernel_entry ) { uint32_t * p ; p = ( uint32_t * ) base ; stl_raw ( p ++ , 0x0bf00160 ) ; stl_raw ( p ++ , 0x00000000 ) ; stl_raw ( base + 0x500 , 0xbfc00580 ) ; stl_raw ( base + 0x504 , 0xbfc0083c ) ; stl_raw ( base + 0x520 , 0xbfc00580 ) ; stl_raw ( base + 0x52c , 0xbfc00800 ) ; stl_raw ( base + 0x534 , 0xbfc00808 ) ; stl_raw ( base + 0x538 , 0xbfc00800 ) ; stl_raw ( base + 0x53c , 0xbfc00800 ) ; stl_raw ( base + 0x540 , 0xbfc00800 ) ; stl_raw ( base + 0x544 , 0xbfc00800 ) ; stl_raw ( base + 0x548 , 0xbfc00800 ) ; stl_raw ( base + 0x54c , 0xbfc00800 ) ; stl_raw ( base + 0x550 , 0xbfc00800 ) ; stl_raw ( base + 0x554 , 0xbfc00800 ) ; p = ( uint32_t * ) ( base + 0x580 ) ; stl_raw ( p ++ , 0x24040002 ) ; stl_raw ( p ++ , 0x3c1d0000 | ( ( ( ENVP_ADDR - 64 ) >> 16 ) & 0xffff ) ) ; stl_raw ( p ++ , 0x37bd0000 | ( ( ENVP_ADDR - 64 ) & 0xffff ) ) ; stl_raw ( p ++ , 0x3c050000 | ( ( ENVP_ADDR >> 16 ) & 0xffff ) ) ; stl_raw ( p ++ , 0x34a50000 | ( ENVP_ADDR & 0xffff ) ) ; stl_raw ( p ++ , 0x3c060000 | ( ( ( ENVP_ADDR + 8 ) >> 16 ) & 0xffff ) ) ; stl_raw ( p ++ , 0x34c60000 | ( ( ENVP_ADDR + 8 ) & 0xffff ) ) ; stl_raw ( p ++ , 0x3c070000 | ( loaderparams . ram_size >> 16 ) ) ; stl_raw ( p ++ , 0x34e70000 | ( loaderparams . ram_size & 0xffff ) ) ; stl_raw ( p ++ , 0x3c09b400 ) ; # ifdef TARGET_WORDS_BIGENDIAN stl_raw ( p ++ , 0x3c08df00 ) ; # else stl_raw ( p ++ , 0x340800df ) ; # endif stl_raw ( p ++ , 0xad280068 ) ; stl_raw ( p ++ , 0x3c09bbe0 ) ; # ifdef TARGET_WORDS_BIGENDIAN stl_raw ( p ++ , 0x3c08c000 ) ; # else stl_raw ( p ++ , 0x340800c0 ) ; # endif stl_raw ( p ++ , 0xad280048 ) ; # ifdef TARGET_WORDS_BIGENDIAN stl_raw ( p ++ , 0x3c084000 ) ; # else stl_raw ( p ++ , 0x34080040 ) ; # endif stl_raw ( p ++ , 0xad280050 ) ; # ifdef TARGET_WORDS_BIGENDIAN stl_raw ( p ++ , 0x3c088000 ) ; # else stl_raw ( p ++ , 0x34080080 ) ; # endif stl_raw ( p ++ , 0xad280058 ) ; # ifdef TARGET_WORDS_BIGENDIAN stl_raw ( p ++ , 0x3c083f00 ) ; # else stl_raw ( p ++ , 0x3408003f ) ; # endif stl_raw ( p ++ , 0xad280060 ) ; # ifdef TARGET_WORDS_BIGENDIAN stl_raw ( p ++ , 0x3c08c100 ) ; # else stl_raw ( p ++ , 0x340800c1 ) ; # endif stl_raw ( p ++ , 0xad280080 ) ; # ifdef TARGET_WORDS_BIGENDIAN stl_raw ( p ++ , 0x3c085e00 ) ; # else stl_raw ( p ++ , 0x3408005e ) ; # endif stl_raw ( p ++ , 0xad280088 ) ; stl_raw ( p ++ , 0x3c1f0000 | ( ( kernel_entry >> 16 ) & 0xffff ) ) ; stl_raw ( p ++ , 0x37ff0000 | ( kernel_entry & 0xffff ) ) ; stl_raw ( p ++ , 0x03e00008 ) ; stl_raw ( p ++ , 0x00000000 ) ; p = ( uint32_t * ) ( base + 0x800 ) ; stl_raw ( p ++ , 0x03e00008 ) ; stl_raw ( p ++ , 0x24020000 ) ; stl_raw ( p ++ , 0x03e06821 ) ; stl_raw ( p ++ , 0x00805821 ) ; stl_raw ( p ++ , 0x00a05021 ) ; stl_raw ( p ++ , 0x91440000 ) ; stl_raw ( p ++ , 0x254a0001 ) ; stl_raw ( p ++ , 0x10800005 ) ; stl_raw ( p ++ , 0x00000000 ) ; stl_raw ( p ++ , 0x0ff0021c ) ; stl_raw ( p ++ , 0x00000000 ) ; stl_raw ( p ++ , 0x08000205 ) ; stl_raw ( p ++ , 0x00000000 ) ; stl_raw ( p ++ , 0x01a00008 ) ; stl_raw ( p ++ , 0x01602021 ) ; stl_raw ( p ++ , 0x03e06821 ) ; stl_raw ( p ++ , 0x00805821 ) ; stl_raw ( p ++ , 0x00a05021 ) ; stl_raw ( p ++ , 0x00c06021 ) ; stl_raw ( p ++ , 0x91440000 ) ; stl_raw ( p ++ , 0x0ff0021c ) ; stl_raw ( p ++ , 0x00000000 ) ; stl_raw ( p ++ , 0x254a0001 ) ; stl_raw ( p ++ , 0x258cffff ) ; stl_raw ( p ++ , 0x1580fffa ) ; stl_raw ( p ++ , 0x00000000 ) ; stl_raw ( p ++ , 0x01a00008 ) ; stl_raw ( p ++ , 0x01602021 ) ; stl_raw ( p ++ , 0x3c08b800 ) ; stl_raw ( p ++ , 0x350803f8 ) ; stl_raw ( p ++ , 0x91090005 ) ; stl_raw ( p ++ , 0x00000000 ) ; stl_raw ( p ++ , 0x31290040 ) ; stl_raw ( p ++ , 0x1120fffc ) ; stl_raw ( p ++ , 0x00000000 ) ; stl_raw ( p ++ , 0x03e00008 ) ; stl_raw ( p ++ , 0xa1040000 ) ; }",
         "0"
        ],
        [
         "261",
         "static bool search_header ( struct message_search_context * ctx , const struct message_header_line * hdr ) { static const unsigned char crlf [ 2 ] = { '\\r' , '\\n' } ; return str_find_more ( ctx -> str_find_ctx , ( const unsigned char * ) hdr -> name , hdr -> name_len ) || str_find_more ( ctx -> str_find_ctx , hdr -> middle , hdr -> middle_len ) || str_find_more ( ctx -> str_find_ctx , hdr -> full_value , hdr -> full_value_len ) || ( ! hdr -> no_newline && str_find_more ( ctx -> str_find_ctx , crlf , 2 ) ) ; }",
         "0"
        ],
        [
         "4192",
         "static int qemuAgentIOProcessLine ( qemuAgentPtr mon , const char * line , qemuAgentMessagePtr msg ) { virJSONValuePtr obj = NULL ; int ret = - 1 ; VIR_DEBUG ( \"Line [%s]\" , line ) ; if ( ! ( obj = virJSONValueFromString ( line ) ) ) { if ( msg && msg -> sync && msg -> first ) { VIR_DEBUG ( \"Received garbage on sync\" ) ; msg -> finished = 1 ; return 0 ; } goto cleanup ; } if ( obj -> type != VIR_JSON_TYPE_OBJECT ) { virReportError ( VIR_ERR_INTERNAL_ERROR , _ ( \"Parsed JSON reply '%s' isn't an object\" ) , line ) ; goto cleanup ; } if ( virJSONValueObjectHasKey ( obj , \"QMP\" ) == 1 ) { ret = 0 ; } else if ( virJSONValueObjectHasKey ( obj , \"event\" ) == 1 ) { ret = qemuAgentIOProcessEvent ( mon , obj ) ; } else if ( virJSONValueObjectHasKey ( obj , \"error\" ) == 1 || virJSONValueObjectHasKey ( obj , \"return\" ) == 1 ) { if ( msg ) { if ( msg -> sync ) { unsigned long long id ; if ( virJSONValueObjectGetNumberUlong ( obj , \"return\" , & id ) < 0 ) { VIR_DEBUG ( \"Ignoring delayed reply on sync\" ) ; ret = 0 ; goto cleanup ; } VIR_DEBUG ( \"Guest returned ID: %llu\" , id ) ; if ( msg -> id != id ) { VIR_DEBUG ( \"Guest agent returned ID: %llu instead of %llu\" , id , msg -> id ) ; ret = 0 ; goto cleanup ; } } msg -> rxObject = obj ; msg -> finished = 1 ; obj = NULL ; } else { VIR_DEBUG ( \"Ignoring delayed reply\" ) ; } ret = 0 ; } else { virReportError ( VIR_ERR_INTERNAL_ERROR , _ ( \"Unknown JSON reply '%s'\" ) , line ) ; } cleanup : virJSONValueFree ( obj ) ; return ret ; }",
         "0"
        ],
        [
         "4548",
         "static char * default_opaque_literal_tag ( tvbuff_t * tvb , guint32 offset , const char * token _U_ , guint8 codepage _U_ , guint32 * length ) { guint32 data_len = tvb_get_guintvar ( tvb , offset , length ) ; char * str = wmem_strdup_printf ( wmem_packet_scope ( ) , \"(%d bytes of opaque data)\" , data_len ) ; * length += data_len ; return str ; }",
         "0"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>func</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11902</th>\n",
       "      <td>OM_uint32 kg_sync_ccache_name ( krb5_context c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11648</th>\n",
       "      <td>static void write_bootloader ( CPUMIPSState * ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>static bool search_header ( struct message_sea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4192</th>\n",
       "      <td>static int qemuAgentIOProcessLine ( qemuAgentP...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4548</th>\n",
       "      <td>static char * default_opaque_literal_tag ( tvb...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    func  target\n",
       "11902  OM_uint32 kg_sync_ccache_name ( krb5_context c...       0\n",
       "11648  static void write_bootloader ( CPUMIPSState * ...       0\n",
       "261    static bool search_header ( struct message_sea...       0\n",
       "4192   static int qemuAgentIOProcessLine ( qemuAgentP...       0\n",
       "4548   static char * default_opaque_literal_tag ( tvb...       0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972a488a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing train set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train set: 100%|██████████| 18187/18187 [00:00<00:00, 20059.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error analyzing code with DeepSeek: invalid literal for int() with base 10: '1\\n\\nThis code contains potential security vulnerabilities, including:\\n1. Lack of proper input validation which could lead to injection attacks\\n2. Potential memory leaks if error paths are taken (t\n",
      "Error analyzing code with DeepSeek: Error code: 400 - {'error': {'message': \"This model's maximum context length is 65536 tokens. However, you requested 84570 tokens (84570 in the messages, 0 in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "Error analyzing code with DeepSeek: invalid literal for int() with base 10: \"This code appears to be part of the MySQL/MyISAM storage engine's repair functionality. After careful analysis, here are the potential security concerns:\\n\\n1. The code uses unsafe functions like `pr\n",
      "Error analyzing code with DeepSeek: invalid literal for int() with base 10: 'This code appears to be part of a database repair function in MySQL (specifically MyISAM storage engine). After analyzing it, here are the potential security concerns:\\n\\n1. There are direct file ope\n",
      "Error analyzing code with DeepSeek: invalid literal for int() with base 10: \"0\\n\\nThe provided code appears to be a protocol dissector for RSVP (Resource Reservation Protocol) in Wireshark. It defines various fields and structures for parsing RSVP messages but doesn't contain\n",
      "Error analyzing code with DeepSeek: invalid literal for int() with base 10: '1\\n\\nThe code contains potential security vulnerabilities, including:\\n1. Integer overflow risks in memory allocations (e.g., `vc->floor_count * sizeof(*vc->floors)`)\\n2. Lack of bounds checking on a\n",
      "Error analyzing code with DeepSeek: invalid literal for int() with base 10: 'This code appears to be part of the MyISAM storage engine in MySQL, specifically handling table repair operations. After analyzing the code, here are the key observations regarding security vulnerabi\n",
      "Error analyzing code with DeepSeek: invalid literal for int() with base 10: '1\\n\\nThe code is vulnerable because it performs a `memcpy` from `config_data` to a `struct virtio_balloon_config` without checking the size of `config_data`. This could lead to a buffer overflow if `\n",
      "Error analyzing code with DeepSeek: invalid literal for int() with base 10: \"1\\n\\nThe code is vulnerable because it does not perform any bounds checking when copying the `utf8` string into `aes->aes_utf8` using `archive_strncpy`. If `utf8` is a very long string, this could le\n",
      "Error analyzing code with DeepSeek: invalid literal for int() with base 10: '1\\n\\nThe code contains several potential security vulnerabilities:\\n\\n1. File handling without proper error checking (fopen operations)\\n2. Potential resource leaks if fclose fails or is not called i\n",
      "Error analyzing code with DeepSeek: invalid literal for int() with base 10: '1\\n\\nThe code contains potential security vulnerabilities, including:\\n1. Lack of proper bounds checking when accessing `buf` array (e.g., `buf[0]` is accessed without checking if `buf_size > 0`)\\n2.\n",
      "Error analyzing code with DeepSeek: invalid literal for int() with base 10: 'This code appears to be part of a SQL command parser and contains several potential security vulnerabilities:\\n\\n1. Buffer overflow risk in the `sprintf(buff, \"Unknown command \\'\\\\\\\\%c\\'.\"` line - us\n",
      "Error analyzing code with DeepSeek: invalid literal for int() with base 10: \"This code appears to be part of the ICU (International Components for Unicode) library, specifically handling locale keyword processing. After analyzing it, I don't see any obvious security vulnerabi\n",
      "Error analyzing code with DeepSeek: invalid literal for int() with base 10: '1\\n\\nThe code contains potential security vulnerabilities:\\n1. No check for malloc/realloc failure which could lead to NULL pointer dereference\\n2. No bounds checking on the input string length (mn->\n",
      "\n",
      "Metrics for train:\n",
      "Accuracy:  0.8183\n",
      "Precision: 0.1345\n",
      "Recall:    0.1539\n",
      "F1 Score:  0.1436\n",
      "\n",
      "Processing val set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing val set: 100%|██████████| 2273/2273 [00:00<00:00, 27674.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for val:\n",
      "Accuracy:  0.8284\n",
      "Precision: 0.1429\n",
      "Recall:    0.1714\n",
      "F1 Score:  0.1558\n",
      "\n",
      "Processing test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test set: 100%|██████████| 2274/2274 [00:00<00:00, 23112.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error analyzing code with DeepSeek: Error code: 400 - {'error': {'message': \"This model's maximum context length is 65536 tokens. However, you requested 84570 tokens (84570 in the messages, 0 in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "\n",
      "Metrics for test:\n",
      "Accuracy:  0.8087\n",
      "Precision: 0.1161\n",
      "Recall:    0.1348\n",
      "F1 Score:  0.1247\n",
      "\n",
      "=== Final Results Summary ===\n",
      "\n",
      "Metrics for train:\n",
      "Accuracy:  0.8183\n",
      "Precision: 0.1345\n",
      "Recall:    0.1539\n",
      "F1 Score:  0.1436\n",
      "\n",
      "TRAIN Set:\n",
      "Accuracy  : 0.8183\n",
      "Precision : 0.1345\n",
      "Recall    : 0.1539\n",
      "F1        : 0.1436\n",
      "\n",
      "Metrics for val:\n",
      "Accuracy:  0.8284\n",
      "Precision: 0.1429\n",
      "Recall:    0.1714\n",
      "F1 Score:  0.1558\n",
      "\n",
      "VAL Set:\n",
      "Accuracy  : 0.8284\n",
      "Precision : 0.1429\n",
      "Recall    : 0.1714\n",
      "F1        : 0.1558\n",
      "\n",
      "Metrics for test:\n",
      "Accuracy:  0.8087\n",
      "Precision: 0.1161\n",
      "Recall:    0.1348\n",
      "F1 Score:  0.1247\n",
      "\n",
      "TEST Set:\n",
      "Accuracy  : 0.8087\n",
      "Precision : 0.1161\n",
      "Recall    : 0.1348\n",
      "F1        : 0.1247\n",
      "\n",
      "Consolidated Metrics:\n",
      "                                                       0\n",
      "train                                                ...\n",
      "val                                                  ...\n",
      "test                                                 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2312985/228042362.py:84: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['deepseek_prediction'] = results\n",
      "/tmp/ipykernel_2312985/228042362.py:88: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['deepseek_match'] = (df['deepseek_prediction'] == df['target']).astype(int)\n",
      "/mnt/data1/aikedaer/anaconda3/envs/ds/lib/python3.8/site-packages/pandas/core/internals/construction.py:553: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import torch\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "client = OpenAI(api_key=\"your_api_key\", base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "def detect_vulnerability(code, gpu_id):\n",
    "    try:\n",
    "        device = torch.device(f'cuda:{gpu_id}' if torch.cuda.is_available() else 'cpu')\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"deepseek-chat\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a cybersecurity expert analyzing code for vulnerabilities. Respond with '1' if vulnerable or '0' if safe.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Does this code contain security vulnerabilities? Respond with only '1' for yes or '0' for no:\\n\\n{code}\"}\n",
    "            ],\n",
    "            stream=False\n",
    "        )\n",
    "        \n",
    "        result = response.choices[0].message.content.strip()\n",
    "        return int(result)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing code with DeepSeek: {e}\")\n",
    "        return 0\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, dataset_name):\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\nMetrics for {dataset_name}:\")\n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "def process_single_task(func, gpu_id):\n",
    "    return detect_vulnerability(func, gpu_id)\n",
    "\n",
    "def process_dataset(df, name, gpu_count=8):\n",
    "    results = []\n",
    "    with ProcessPoolExecutor(max_workers=gpu_count) as executor:\n",
    "        futures = []\n",
    "        \n",
    "        for idx, func in enumerate(tqdm(df['func'], desc=f\"Processing {name} set\", position=0, leave=True)):\n",
    "            gpu_id = idx % gpu_count\n",
    "            futures.append(executor.submit(process_single_task, func, gpu_id))\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "\n",
    "    df['deepseek_prediction'] = results\n",
    "    \n",
    "    if 'target' in df.columns:\n",
    "        metrics = calculate_metrics(df['target'], df['deepseek_prediction'], name)\n",
    "        df['deepseek_match'] = (df['deepseek_prediction'] == df['target']).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def process_datasets_sequentially(datasets):\n",
    "    results = {}\n",
    "    for idx, (df, name) in enumerate(zip(datasets, ['test'])):\n",
    "        print(f\"\\nProcessing {name} set...\")\n",
    "        results[name] = process_dataset(df, name)\n",
    "    \n",
    "    return results\n",
    "\n",
    "datasets = [m3]\n",
    "results = process_datasets_sequentially(datasets)\n",
    "\n",
    "results['test'].to_pickle(f'../../data/finetune/{dataset}/{dataset}_test_with_deepseek.pkl')\n",
    "\n",
    "print(\"\\n=== Final Results Summary ===\")\n",
    "for name, df in results.items():\n",
    "    metrics = calculate_metrics(df['target'], df['deepseek_prediction'], name)\n",
    "    print(f\"\\n{name.upper()} Set:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric.capitalize():<10}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
