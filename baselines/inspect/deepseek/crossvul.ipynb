{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f2e284d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clang import *\n",
    "from tokenizers import NormalizedString,PreTokenizedString\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers import Tokenizer, normalizers\n",
    "from tokenizers.pre_tokenizers import PreTokenizer\n",
    "from tokenizers.normalizers import StripAccents,Replace\n",
    "from tokenizers import processors\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "import threading\n",
    "from typing import List\n",
    "\n",
    "class MyTokenizer:\n",
    "    cidx = cindex.Index.create()\n",
    "\n",
    "    def __init__(self, timeout=5):  # 设置超时时间\n",
    "        self.timeout = timeout\n",
    "\n",
    "    def clang_split(self, i: int, normalized_string: NormalizedString) -> List[NormalizedString]:\n",
    "        \"\"\"\n",
    "        使用 Clang 对代码进行分词，增加超时机制\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        exception = None\n",
    "\n",
    "        def parse():\n",
    "            nonlocal result, exception\n",
    "            try:\n",
    "                tok = []\n",
    "                tu = self.cidx.parse(\n",
    "                    'tmp.c',\n",
    "                    args=[''],  \n",
    "                    unsaved_files=[('tmp.c', str(normalized_string.original))],  \n",
    "                    options=0\n",
    "                )\n",
    "                for t in tu.get_tokens(extent=tu.cursor.extent):\n",
    "                    spelling = t.spelling.strip()\n",
    "                    if spelling == '':\n",
    "                        continue\n",
    "                    tok.append(NormalizedString(spelling))\n",
    "                result = tok\n",
    "            except Exception as e:\n",
    "                exception = e\n",
    "\n",
    "        # 创建线程\n",
    "        thread = threading.Thread(target=parse)\n",
    "        thread.start()\n",
    "        thread.join(self.timeout)  # 等待超时时间\n",
    "\n",
    "        if thread.is_alive():  # 超时检查\n",
    "            print(f\"Timeout occurred while parsing: {normalized_string.original[:100]}...\")\n",
    "            thread.join(0)  # 跳过此任务\n",
    "            return []\n",
    "        if exception:\n",
    "            print(f\"Error during Clang parsing: {exception}\")\n",
    "            return []\n",
    "\n",
    "        return result\n",
    "\n",
    "    def pre_tokenize(self, pretok: PreTokenizedString):\n",
    "        \"\"\"\n",
    "        对预分词字符串进行处理，调用 Clang 分词器\n",
    "        \"\"\"\n",
    "        def preprocess_and_split(i: int, normalized_string: NormalizedString) -> List[NormalizedString]:\n",
    "            return self.clang_split(i, normalized_string)\n",
    "        \n",
    "        pretok.split(preprocess_and_split)\n",
    "\n",
    "import re\n",
    "def cleaner(code):\n",
    "    pat = re.compile(r'(/\\*([^*]|(\\*+[^*/]))*\\*+/)|(//.*)')\n",
    "    code = re.sub(pat, '', code)\n",
    "    code = re.sub('\\n', '', code)\n",
    "    code = re.sub('\\t', '', code)\n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb49c68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = \"crossvul\"\n",
    "m1 = pd.read_pickle(f'../../data/finetune/{dataset}/{dataset}_train.pkl')\n",
    "m2 = pd.read_pickle(f'../../data/finetune/{dataset}/{dataset}_val.pkl')\n",
    "m3 = pd.read_pickle(f'../../data/finetune/{dataset}/{dataset}_test.pkl')\n",
    "\n",
    "for df in [m1, m2, m3]:\n",
    "    if \"functionSource\" in df.columns:\n",
    "        df[\"func\"] = df[\"functionSource\"].apply(cleaner)\n",
    "        \n",
    "    if dataset == \"draper\":\n",
    "        df[\"target\"] = df[\"combine\"] * 1\n",
    "\n",
    "    if \"label\" in df.columns:\n",
    "        df[\"target\"] = df[\"label\"]\n",
    "\n",
    "    if dataset == \"mvd\":\n",
    "        df[\"target\"] = df[\"target\"].apply(lambda x: 1 if x != 0 else 0)\n",
    "\n",
    "m1 = m1[[\"func\", \"target\"]]\n",
    "m2 = m2[[\"func\", \"target\"]]\n",
    "m3 = m3[[\"func\", \"target\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "812b38d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "func",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "target",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "3fa836fe-b681-4831-89d7-a920da7ef248",
       "rows": [
        [
         "0",
         "static int ntop_interface_store_alert(lua_State* vm) {\n  int ifid;\n  NetworkInterface* iface;\n  AlertsManager *am;\n\n  ntop->getTrace()->traceEvent(TRACE_DEBUG, \"%s() called\", __FUNCTION__);\n\n  if(ntop_lua_check(vm, __FUNCTION__, 1, LUA_TNUMBER)) return(CONST_LUA_ERROR);\n  if(ntop_lua_check(vm, __FUNCTION__, 2, LUA_TTABLE)) return(CONST_LUA_ERROR);\n\n  ifid = lua_tointeger(vm, 1);\n  if(ifid < 0)\n    return(CONST_LUA_ERROR);\n\n  if(!(iface = ntop->getNetworkInterface(vm, ifid)) ||\n     !(am = iface->getAlertsManager()))\n    return (CONST_LUA_ERROR);\n\n  return am->storeAlert(vm, 2) ? CONST_LUA_ERROR : CONST_LUA_OK;\n}",
         "0"
        ],
        [
         "1",
         "static socklen_t get_peername(h2o_conn_t *_conn, struct sockaddr *sa)\n{\n    h2o_http2_conn_t *conn = (void *)_conn;\n    return h2o_socket_getpeername(conn->sock, sa);\n}\n",
         "0"
        ],
        [
         "2",
         "static int connect(struct socket *sock, struct sockaddr *dest, int destlen,\n\t\t   int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct sockaddr_tipc *dst = (struct sockaddr_tipc *)dest;\n\tstruct msghdr m = {NULL,};\n\tunsigned int timeout;\n\tint res;\n\n\tlock_sock(sk);\n\n\t/* For now, TIPC does not allow use of connect() with DGRAM/RDM types */\n\tif (sock->state == SS_READY) {\n\t\tres = -EOPNOTSUPP;\n\t\tgoto exit;\n\t}\n\n\t/*\n\t * Reject connection attempt using multicast address\n\t *\n\t * Note: send_msg() validates the rest of the address fields,\n\t *       so there's no need to do it here\n\t */\n\tif (dst->addrtype == TIPC_ADDR_MCAST) {\n\t\tres = -EINVAL;\n\t\tgoto exit;\n\t}\n\n\ttimeout = (flags & O_NONBLOCK) ? 0 : tipc_sk(sk)->conn_timeout;\n\n\tswitch (sock->state) {\n\tcase SS_UNCONNECTED:\n\t\t/* Send a 'SYN-' to destination */\n\t\tm.msg_name = dest;\n\t\tm.msg_namelen = destlen;\n\n\t\t/* If connect is in non-blocking case, set MSG_DONTWAIT to\n\t\t * indicate send_msg() is never blocked.\n\t\t */\n\t\tif (!timeout)\n\t\t\tm.msg_flags = MSG_DONTWAIT;\n\n\t\tres = send_msg(NULL, sock, &m, 0);\n\t\tif ((res < 0) && (res != -EWOULDBLOCK))\n\t\t\tgoto exit;\n\n\t\t/* Just entered SS_CONNECTING state; the only\n\t\t * difference is that return value in non-blocking\n\t\t * case is EINPROGRESS, rather than EALREADY.\n\t\t */\n\t\tres = -EINPROGRESS;\n\t\tbreak;\n\tcase SS_CONNECTING:\n\t\tres = -EALREADY;\n\t\tbreak;\n\tcase SS_CONNECTED:\n\t\tres = -EISCONN;\n\t\tbreak;\n\tdefault:\n\t\tres = -EINVAL;\n\t\tgoto exit;\n\t}\n\n\tif (sock->state == SS_CONNECTING) {\n\t\tif (!timeout)\n\t\t\tgoto exit;\n\n\t\t/* Wait until an 'ACK' or 'RST' arrives, or a timeout occurs */\n\t\trelease_sock(sk);\n\t\tres = wait_event_interruptible_timeout(*sk_sleep(sk),\n\t\t\t\tsock->state != SS_CONNECTING,\n\t\t\t\ttimeout ? (long)msecs_to_jiffies(timeout)\n\t\t\t\t\t: MAX_SCHEDULE_TIMEOUT);\n\t\tlock_sock(sk);\n\t\tif (res <= 0) {\n\t\t\tif (res == 0)\n\t\t\t\tres = -ETIMEDOUT;\n\t\t\telse\n\t\t\t\t; /* leave \"res\" unchanged */\n\t\t\tgoto exit;\n\t\t}\n\t}\n\n\tif (unlikely(sock->state == SS_DISCONNECTING))\n\t\tres = sock_error(sk);\n\telse\n\t\tres = 0;\n\nexit:\n\trelease_sock(sk);\n\treturn res;\n}",
         "0"
        ],
        [
         "3",
         "R_API RBinFile *r_bin_file_find_by_name_n(RBin *bin, const char *name, int idx) {\n\tRListIter *iter;\n\tRBinFile *bf = NULL;\n\tint i = 0;\n\tif (!bin) {\n\t\treturn bf;\n\t}\n\n\tr_list_foreach (bin->binfiles, iter, bf) {\n\t\tif (bf && bf->file && !strcmp (bf->file, name)) {\n\t\t\tif (i == idx) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\ti++;\n\t\t}\n\t\tbf = NULL;\n\t}\n\treturn bf;\n}",
         "0"
        ],
        [
         "4",
         "size_t cac_list_meter(const void *el) {\n\treturn sizeof(cac_object_t);\n}",
         "0"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>func</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>static int ntop_interface_store_alert(lua_Stat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>static socklen_t get_peername(h2o_conn_t *_con...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>static int connect(struct socket *sock, struct...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R_API RBinFile *r_bin_file_find_by_name_n(RBin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>size_t cac_list_meter(const void *el) {\\n\\tret...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                func  target\n",
       "0  static int ntop_interface_store_alert(lua_Stat...       0\n",
       "1  static socklen_t get_peername(h2o_conn_t *_con...       0\n",
       "2  static int connect(struct socket *sock, struct...       0\n",
       "3  R_API RBinFile *r_bin_file_find_by_name_n(RBin...       0\n",
       "4  size_t cac_list_meter(const void *el) {\\n\\tret...       0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e85e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test set:   0%|          | 0/13305 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test set: 100%|██████████| 13305/13305 [00:00<00:00, 18340.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for test:\n",
      "Accuracy:  0.8831\n",
      "Precision: 0.1021\n",
      "Recall:    0.1606\n",
      "F1 Score:  0.1249\n",
      "\n",
      "=== Final Results Summary ===\n",
      "\n",
      "Metrics for test:\n",
      "Accuracy:  0.8831\n",
      "Precision: 0.1021\n",
      "Recall:    0.1606\n",
      "F1 Score:  0.1249\n",
      "\n",
      "TEST Set:\n",
      "Accuracy  : 0.8831\n",
      "Precision : 0.1021\n",
      "Recall    : 0.1606\n",
      "F1        : 0.1249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3698436/454091750.py:84: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['deepseek_prediction'] = results\n",
      "/tmp/ipykernel_3698436/454091750.py:88: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['deepseek_match'] = (df['deepseek_prediction'] == df['target']).astype(int)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Must pass 2-d input. shape=(1, 13305, 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3698436/454091750.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;31m# 合并所有指标的 DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m \u001b[0mmetrics_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nConsolidated Metrics:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/data1/aikedaer/anaconda3/envs/ds/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mfrom_dict\u001b[0;34m(cls, data, orient, dtype, columns)\u001b[0m\n\u001b[1;32m   1758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1759\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"tight\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1760\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1761\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1762\u001b[0m             \u001b[0mrealdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/data1/aikedaer/anaconda3/envs/ds/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    796\u001b[0m                     )\n\u001b[1;32m    797\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m                     mgr = ndarray_to_mgr(\n\u001b[0m\u001b[1;32m    799\u001b[0m                         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m                         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/data1/aikedaer/anaconda3/envs/ds/lib/python3.8/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;31m# by definition an array here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;31m# the dtypes will be coerced to a single dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prep_ndarraylike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy_on_sanitize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_dtype_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/data1/aikedaer/anaconda3/envs/ds/lib/python3.8/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_prep_ndarraylike\u001b[0;34m(values, copy)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ensure_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/data1/aikedaer/anaconda3/envs/ds/lib/python3.8/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_ensure_2d\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Must pass 2-d input. shape={values.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Must pass 2-d input. shape=(1, 13305, 4)"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import torch\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "client = OpenAI(api_key=\"your_api_key\", base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "def detect_vulnerability(code, gpu_id):\n",
    "    try:\n",
    "        device = torch.device(f'cuda:{gpu_id}' if torch.cuda.is_available() else 'cpu')\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"deepseek-chat\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a cybersecurity expert analyzing code for vulnerabilities. Respond with '1' if vulnerable or '0' if safe.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Does this code contain security vulnerabilities? Respond with only '1' for yes or '0' for no:\\n\\n{code}\"}\n",
    "            ],\n",
    "            stream=False\n",
    "        )\n",
    "        \n",
    "        result = response.choices[0].message.content.strip()\n",
    "        return int(result)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return 0\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, dataset_name):\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\nMetrics for {dataset_name}:\")\n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "def process_single_task(func, gpu_id):\n",
    "    return detect_vulnerability(func, gpu_id)\n",
    "\n",
    "def process_dataset(df, name, gpu_count=8):\n",
    "    results = []\n",
    "    with ProcessPoolExecutor(max_workers=gpu_count) as executor:\n",
    "        futures = []\n",
    "        \n",
    "        for idx, func in enumerate(tqdm(df['func'], desc=f\"Processing {name} set\", position=0, leave=True)):\n",
    "            gpu_id = idx % gpu_count\n",
    "            futures.append(executor.submit(process_single_task, func, gpu_id))\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "    \n",
    "    df['deepseek_prediction'] = results\n",
    "    \n",
    "    if 'target' in df.columns:\n",
    "        metrics = calculate_metrics(df['target'], df['deepseek_prediction'], name)\n",
    "        df['deepseek_match'] = (df['deepseek_prediction'] == df['target']).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def process_datasets_sequentially(datasets):\n",
    "    results = {}\n",
    "    for idx, (df, name) in enumerate(zip(datasets, ['test'])):\n",
    "        print(f\"\\nProcessing {name} set...\")\n",
    "        results[name] = process_dataset(df, name)\n",
    "    \n",
    "    return results\n",
    "\n",
    "datasets = [m3]\n",
    "results = process_datasets_sequentially(datasets)\n",
    "\n",
    "results['test'].to_pickle(f'../../data/finetune/{dataset}/{dataset}_test_with_deepseek.pkl')\n",
    "\n",
    "print(\"\\n=== Final Results Summary ===\")\n",
    "for name, df in results.items():\n",
    "    metrics = calculate_metrics(df['target'], df['deepseek_prediction'], name)\n",
    "    print(f\"\\n{name.upper()} Set:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric.capitalize():<10}: {value:.4f}\")\n",
    "\n",
    "metrics_df = pd.DataFrame.from_dict(results, orient='index')\n",
    "print(\"\\nConsolidated Metrics:\")\n",
    "print(metrics_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
